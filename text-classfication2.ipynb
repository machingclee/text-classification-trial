{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_business = os.path.sep.join([\"data_set\",\"bbc\",\"business\"])\n",
    "bbc = os.path.sep.join([\"data_set\",\"bbc\"]) \n",
    "WORD_EMBEDDING_DIMENSION = 50\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 500\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510 files in data_set\\bbc\\business have been loaded\n",
      "386 files in data_set\\bbc\\entertainment have been loaded\n",
      "417 files in data_set\\bbc\\politics have been loaded\n",
      "511 files in data_set\\bbc\\sport have been loaded\n",
      "401 files in data_set\\bbc\\tech have been loaded\n"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "labels = []\n",
    "\n",
    "def preprocess_data(folder_path):\n",
    "    for i, (dir_path, dir_names, file_names) in enumerate(os.walk(folder_path)):\n",
    "        if dir_path != os.path.sep.join([\"data_set\",\"bbc\"]):\n",
    "            print(f\"{len(file_names)} files in {dir_path} have been loaded\")\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.sep.join([dir_path, file_name])\n",
    "                category = file_path.split(os.path.sep)[-2]\n",
    "                with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "                    content = f.read().strip()\n",
    "                    for punc in string.punctuation:\n",
    "                        content = content.replace(punc,\"\")\n",
    "                    content = re.sub(r\"(\\n)+\", \" \", content)\n",
    "                    content = content.lower()\n",
    "                    \n",
    "                    contents.append(content)\n",
    "                    labels.append(category)\n",
    "                    \n",
    "preprocess_data(bbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have total of 2225 training data\n",
      "1764 of paragraph has number of words less than 500\n",
      "1982 of paragraph has number of words less than 600\n",
      "2095 of paragraph has number of words less than 700\n",
      "2146 of paragraph has number of words less than 800\n",
      "2191 of paragraph has number of words less than 900\n",
      "2203 of paragraph has number of words less than 1000\n",
      "2210 of paragraph has number of words less than 1200\n",
      "2216 of paragraph has number of words less than 1400\n",
      "2217 of paragraph has number of words less than 1600\n",
      "max number of words: 4416\n",
      "min number of words: 89\n",
      "number of words: 851028\n",
      "average number of words: 382\n"
     ]
    }
   ],
   "source": [
    "print(f\"we have total of {len(contents)} training data\")\n",
    "\n",
    "nums=np.array([len(content.split()) for content in contents])\n",
    "\n",
    "max_num_of_words = np.max(nums)\n",
    "min_num_of_words = np.min(nums)\n",
    "total_num_of_words = np.sum(nums)\n",
    "average_num_of_words = total_num_of_words//len(contents)\n",
    "\n",
    "for threshold in [500,600,700,800,900,1000,1200,1400,1600]:\n",
    "    print(f\"{len([num for num in nums if num < threshold])} of paragraph has number of words less than {threshold}\")\n",
    "\n",
    "print(f\"max number of words: {max_num_of_words}\")\n",
    "print(f\"min number of words: {min_num_of_words}\")\n",
    "print(f\"number of words: {total_num_of_words}\")\n",
    "print(f\"average number of words: {average_num_of_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(contents, labels, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelBinarizer = LabelBinarizer()\n",
    "Y_train = labelBinarizer.fit_transform(Y_train)\n",
    "Y_test = labelBinarizer.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "training_word_to_index = tokenizer.word_index\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test =  tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = np.array(sequence.pad_sequences(sequences_train, maxlen=MAX_LENGTH, padding='post'))\n",
    "X_test = np.array(sequence.pad_sequences(sequences_test, maxlen=MAX_LENGTH, padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def define_embedding_layer():\n",
    "    print(\"Loading word vectors...\")\n",
    "    word_to_vec = {}\n",
    "    embedding_file_path = os.path.sep.join([\"word_embedding\", \"glove.6B.{}d.txt\".format(WORD_EMBEDDING_DIMENSION)])\n",
    "    with open(embedding_file_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.array(values[1:], dtype=\"float32\")\n",
    "            word_to_vec[word] = vec\n",
    "    \n",
    "    vocab_size = max(\n",
    "        MAX_VOCAB_SIZE,\n",
    "        len(training_word_to_index) + 1\n",
    "    )\n",
    "    embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIMENSION))\n",
    "\n",
    "    # for embedding matrix, we are just interested in words in our training set:\n",
    "    for word, index in training_word_to_index.items():\n",
    "        word_vec = word_to_vec.get(word)\n",
    "        if word_vec is not None:\n",
    "            embedding_matrix[index] = word_vec\n",
    "\n",
    "    training_word_embedding_layer = Embedding(\n",
    "        vocab_size,\n",
    "        WORD_EMBEDDING_DIMENSION,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return encoder_word_embedding_layer\n",
    "\n",
    "training_word_embedding_layer = define_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2855   11    1 2856 1317  343 5963   70    3    1  186   10  506   15\n",
      "  633 4541 3831   37  784    1  704 2126    1  113   49    8    5 2648\n",
      "    9  395    8  786  147 8250    4 4000 5964  133    2  270    2 4001\n",
      "  398 6385  393   55  313    6 1172   52    3  213   49    8   82  270\n",
      " 3326  339 2855   83  113  253 1674  591  453   15 1522 4747 6896   60\n",
      "  210 3832 6385   99   31 1030  270   23   15 2261 4164 6896  268    9\n",
      "   12   35  412    2   92   44  317  139   16    1  786 4360   72  186\n",
      "  441 8251  186    2  271 1675   10  398 4967  537   10 5598    9   19\n",
      "  787    6    1  315 4542 9181   40 7479 6897   99 5965  270 1052  249\n",
      "  398    2  335 4543    4 4361 1675   22 1676    4  321  335  597  704\n",
      "    2  271 5247    7  330 2921    3 6385 2033   59  182 1916 1029   43\n",
      " 3549 2310 1586   23    9   13    5 1766 1917   36   13 3103  167   17\n",
      "   12   13   26  260   17   87    6    1 9182 1658    5  252   11    1\n",
      " 1767    3    1   46 1545   13    2  788    5 3550  858    3 2034   90\n",
      "   17 2572  857  206 1586 4164 3327 1016   53 8251 5248   53 7478 1725\n",
      "    2 5599   16  160 1017   60  474    6 5966   87 9180   10    1 2573\n",
      "    3 6898 1029  339 2855   11 6898  118   14  129  194  289   23 1198\n",
      "   19    5  932   48    9  550  968    3    1   46 1226   21   26   14\n",
      "  933  376    5  344    8 1338    6    1  101   66  389    5  252   11\n",
      "    1  634  820   19  969   16   25 4541  373  643  369   25    1 1173\n",
      "  181 4541 1099 1464   11   12    8   97 1839   69   34   19  250    8\n",
      "    5 9183  324    3 8252 1412 1545 2922   25 1413  204 4541   11    1\n",
      "   46  820 6899   34   35   91    2  150 5964 1484    5  523    4  128\n",
      "   19   12   17    5 1442    3    5  270  705  128   58  247  170   58\n",
      "  835   58  247   69    1 1227  398    8 3002   58  150   44   84 4362\n",
      "   36  128  687 2366   29   11   15   37   38   83   22 1613  162 3104\n",
      " 9184   49   13   26  432   67    2 1636    1  344  156  724   99    1\n",
      "  254  132   36    8  261    2  107  214    6  118    1  970   19  290\n",
      "    7   33 2574 2575  147 1029   10  398    2   14   96  266 9185  118\n",
      " 1135  343    7    1  677   83  113  253 1674  591  453    9    1  634\n",
      "  186   41 5600    4   41   61    1    3    1  345  216   74   11    5\n",
      "  744   75   35  764    5 4544 1366    3    1  677 1587   17  109   17\n",
      " 9186    5  335 8253    3    4 4164 6896   34  146    2   84 1587    1\n",
      "   62 3695  457   54    1   64 3695   74  110    1   75  114  488  611\n",
      "  765   14 4363   45   91 1465   63    5 5249    1  653 1185   19  953\n",
      "    7 3551    6 4165  398  542  457   54  390 1339]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = Input(name='inputs',shape=(MAX_LENGTH,))\n",
    "    x = training_word_embedding_layer(inputs)\n",
    "    x = LSTM(128)(x)\n",
    "    x = Dense(256,name='FC1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "#     x = LSTM(128)(x)\n",
    "#     x = Dense(256,name='FC2')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.5)(x) \n",
    "\n",
    "    x = Dense(NUM_CLASSES, name='out_layer')(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(model, to_file='model1.png')\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=RMSprop(), \n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1891, 500)\n",
      "1891\n",
      "(334, 500)\n",
      "334\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(Y_train))\n",
    "print(X_test.shape)\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 4s 61ms/step - loss: 1.8171 - acc: 0.2489 - val_loss: 1.6184 - val_acc: 0.1856\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.6549 - acc: 0.2622 - val_loss: 1.6306 - val_acc: 0.1766\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.6335 - acc: 0.2728 - val_loss: 1.6681 - val_acc: 0.1796\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.5309 - acc: 0.3143 - val_loss: 1.6496 - val_acc: 0.1826\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.4773 - acc: 0.3205 - val_loss: 1.6190 - val_acc: 0.1796\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.4243 - acc: 0.3748 - val_loss: 1.6429 - val_acc: 0.1826\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.4058 - acc: 0.3382 - val_loss: 1.6140 - val_acc: 0.2695\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.3961 - acc: 0.3441 - val_loss: 1.5773 - val_acc: 0.2665\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.3562 - acc: 0.3903 - val_loss: 1.6597 - val_acc: 0.2096\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.3360 - acc: 0.3895 - val_loss: 1.5534 - val_acc: 0.2904\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.3170 - acc: 0.3856 - val_loss: 1.5567 - val_acc: 0.2395\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2996 - acc: 0.3926 - val_loss: 1.5965 - val_acc: 0.2874\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.3004 - acc: 0.4071 - val_loss: 6.4804 - val_acc: 0.1677\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.3242 - acc: 0.4078 - val_loss: 1.7033 - val_acc: 0.2575\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.3234 - acc: 0.3675 - val_loss: 2.5934 - val_acc: 0.2605\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.2793 - acc: 0.4024 - val_loss: 1.8676 - val_acc: 0.2934\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 1.2741 - acc: 0.4173 - val_loss: 1.9358 - val_acc: 0.2904\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2671 - acc: 0.4005 - val_loss: 4.4977 - val_acc: 0.1796\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.2978 - acc: 0.4093 - val_loss: 8.9666 - val_acc: 0.1677\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2813 - acc: 0.4028 - val_loss: 7.1256 - val_acc: 0.1677\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.3045 - acc: 0.4068 - val_loss: 1.8446 - val_acc: 0.3084\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2761 - acc: 0.4226 - val_loss: 2.2519 - val_acc: 0.2964\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2779 - acc: 0.4203 - val_loss: 6.7429 - val_acc: 0.2275\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.3051 - acc: 0.3897 - val_loss: 4.0157 - val_acc: 0.2605\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2449 - acc: 0.4240 - val_loss: 2.3811 - val_acc: 0.2006\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2620 - acc: 0.4152 - val_loss: 7.2503 - val_acc: 0.1916\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.2786 - acc: 0.4233 - val_loss: 2.8700 - val_acc: 0.2126\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2323 - acc: 0.4405 - val_loss: 7.3232 - val_acc: 0.2665\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2828 - acc: 0.4102 - val_loss: 25.6906 - val_acc: 0.1677\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.2369 - acc: 0.4303 - val_loss: 5.1480 - val_acc: 0.1916\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.1823 - acc: 0.4820 - val_loss: 4.4539 - val_acc: 0.2066\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.1057 - acc: 0.4888 - val_loss: 14.1212 - val_acc: 0.1677\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9061 - acc: 0.5934 - val_loss: 19.4495 - val_acc: 0.1677\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.7958 - acc: 0.6286 - val_loss: 13.1365 - val_acc: 0.1677\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.5378 - acc: 0.7950 - val_loss: 16.0184 - val_acc: 0.2186\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.4207 - acc: 0.8401 - val_loss: 10.1299 - val_acc: 0.3204\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.2766 - acc: 0.9126 - val_loss: 4.2926 - val_acc: 0.3533\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.1671 - acc: 0.9513 - val_loss: 2.6026 - val_acc: 0.5030\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0967 - acc: 0.9740 - val_loss: 9.6263 - val_acc: 0.1916\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0687 - acc: 0.9822 - val_loss: 2.0063 - val_acc: 0.6138\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.0398 - acc: 0.9917 - val_loss: 11.4770 - val_acc: 0.3802\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0543 - acc: 0.9871 - val_loss: 0.6307 - val_acc: 0.8204\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0201 - acc: 0.9958 - val_loss: 0.5486 - val_acc: 0.8563\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.4665 - val_acc: 0.8982\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0186 - acc: 0.9947 - val_loss: 0.3085 - val_acc: 0.9132\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0081 - acc: 0.9978 - val_loss: 6.5442 - val_acc: 0.4072\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0441 - acc: 0.9865 - val_loss: 0.2472 - val_acc: 0.9341\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 5.6680 - val_acc: 0.4790\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0203 - acc: 0.9968 - val_loss: 0.4300 - val_acc: 0.9192\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 9.4655e-04 - acc: 1.0000 - val_loss: 0.3618 - val_acc: 0.8802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x230772630d0>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          Y_train,\n",
    "          validation_data = (X_test, Y_test),\n",
    "          batch_size=64,\n",
    "          epochs=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./output/classifier.hdf5\")\n",
    "plot_model(model, to_file='model2.png')\n",
    "# saving\n",
    "with open('./output/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./output/classifier.hdf5\")\n",
    "\n",
    "tokenizer = None\n",
    "with open('./output/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_index_word = {}\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    tokenizer_index_word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_paragraph_category(paragraph):\n",
    "    seq = np.array(sequence.pad_sequences(tokenizer.texts_to_sequences([paragraph]), maxlen=MAX_LENGTH, padding='post'))\n",
    "    probabilities = model.predict(seq)\n",
    "    return labelBinarizer.classes_[np.argmax(probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'business'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_paragraph_category(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "prediction business\n",
      "answer business\n",
      "------------\n",
      "354\n",
      "prediction business\n",
      "answer business\n",
      "------------\n",
      "772\n",
      "prediction entertainment\n",
      "answer entertainment\n",
      "------------\n",
      "816\n",
      "prediction entertainment\n",
      "answer entertainment\n",
      "------------\n",
      "468\n",
      "prediction business\n",
      "answer business\n",
      "------------\n",
      "982\n",
      "prediction politics\n",
      "answer politics\n",
      "------------\n",
      "51\n",
      "prediction business\n",
      "answer business\n",
      "------------\n",
      "828\n",
      "prediction entertainment\n",
      "answer entertainment\n",
      "------------\n",
      "536\n",
      "prediction entertainment\n",
      "answer entertainment\n",
      "------------\n",
      "672\n",
      "prediction entertainment\n",
      "answer entertainment\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for index in random.sample(range(0, 2000), 10):\n",
    "    content = contents[index]\n",
    "    label = labels[index]\n",
    "    print(index)\n",
    "    print(\"prediction\", predict_paragraph_category(content))\n",
    "    print(\"answer\", label)\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.4-cuda11",
   "language": "python",
   "name": "tensorflow-2.4-cuda11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
